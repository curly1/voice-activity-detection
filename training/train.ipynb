{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "project_dir = \"/Users/joannarownicka/software/vad\"\n",
    "os.chdir(project_dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from lhotse.cut import CutSet\n",
    "from lhotse.dataset.sampling import SingleCutSampler, BucketingSampler\n",
    "from lhotse.dataset.vad import VadDataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from preprocessing import prepare_vad_dataset\n",
    "from models.dnn import DNN\n",
    "from models.accuracy import compute_acc_without_pad"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "root_dir = Path('evaluation/data')\n",
    "corpus_dir = root_dir / 'vad_data/'\n",
    "output_dir = root_dir / 'vad_data_nb/'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "cuts = CutSet.from_json(output_dir / 'cuts_80_data_augment.json.gz')\n",
    "cuts.describe()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cuts count: 4785\n",
      "Total duration (hours): 16.5\n",
      "Speech duration (hours): 13.3 (80.6%)\n",
      "***\n",
      "Duration statistics (seconds):\n",
      "mean    12.4\n",
      "std      4.0\n",
      "min      1.3\n",
      "25%     11.0\n",
      "50%     13.7\n",
      "75%     15.2\n",
      "max     19.1\n",
      "dtype: float64\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Shuffle data but keep seed fixed, split into 80/10/10\n",
    "#cuts_train, cuts_dev_eval = train_test_split(cuts, train_size=0.8, random_state=0)\n",
    "#cuts_dev, cuts_eval = train_test_split(cuts_dev_eval, train_size=0.5, random_state=0)\n",
    "\n",
    "cuts = cuts.shuffle()\n",
    "vad_manifests = prepare_vad_dataset.prepare_vad_dataset(corpus_dir, output_dir)\n",
    "\n",
    "train_ratio = 0.8\n",
    "\n",
    "num_total = len(vad_manifests[\"supervisions\"])\n",
    "stop_train_idx = int(np.floor(num_total * train_ratio))\n",
    "stop_dev_idx = int((num_total - stop_train_idx) // 2 + stop_train_idx)\n",
    "\n",
    "train_ids, dev_ids, eval_ids = [], [], []\n",
    "counter = 0\n",
    "for sup_seg in vad_manifests[\"supervisions\"]:\n",
    "    id = sup_seg.to_dict()[\"id\"]\n",
    "    if counter < stop_train_idx:\n",
    "        train_ids.append(id)\n",
    "    elif counter < stop_dev_idx:\n",
    "        dev_ids.append(id)\n",
    "    else:\n",
    "        eval_ids.append(id)\n",
    "    counter += 1\n",
    "\n",
    "assert train_ids[-1] != dev_ids[0]\n",
    "assert dev_ids[-1] != eval_ids[0]\n",
    "\n",
    "cuts_train = cuts.subset(supervision_ids=train_ids)\n",
    "cuts_dev = cuts.subset(supervision_ids=dev_ids)\n",
    "cuts_eval = cuts.subset(supervision_ids=eval_ids)\n",
    "\n",
    "cuts_eval.to_json(output_dir / 'cuts_80_eval_data_augment.json.gz')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "vad_dataset = VadDataset()\n",
    "\n",
    "train_sampler = SingleCutSampler(cuts_train, shuffle=False, max_duration=300)\n",
    "dev_sampler = SingleCutSampler(cuts_dev, shuffle=False, max_duration=300)\n",
    "\n",
    "train_dloader = DataLoader(vad_dataset, sampler=train_sampler, batch_size=None)\n",
    "dev_dloader = DataLoader(vad_dataset, sampler=dev_sampler, batch_size=None)\n",
    "\n",
    "cut_ids = next(iter(dev_sampler))\n",
    "sample = vad_dataset[cut_ids]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "learning_rate = 1e-2\n",
    "weight_decay = 1e-4\n",
    "num_epochs = 15\n",
    "\n",
    "model_id = \"dnn_ce_fbank80_ignoreindex_lr2_data_augment\"\n",
    "input_size = sample['inputs'][0].shape[1]\n",
    "log_dir = Path('storage/models') / model_id\n",
    "\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = DNN(input_size=input_size, hidden_size=256, num_classes=2).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "#optim = torch.optim.Adam(model.parameters())\n",
    "#                       lr=learning_rate, weight_decay=weight_decay)\n",
    "                        \n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                        weight_decay=weight_decay)\n",
    "\n",
    "train_acc = []\n",
    "valid_acc = []\n",
    "for epoch in range(num_epochs):\n",
    "    # training\n",
    "    acc = []\n",
    "    model.train()\n",
    "    train_dloader.sampler.set_epoch(epoch)\n",
    "\n",
    "    for batch_idx, data in enumerate(train_dloader):\n",
    "\n",
    "        inputs = data[\"inputs\"].reshape(-1,input_size)\n",
    "        targets = data[\"is_voice\"].reshape(-1,1).view(-1)\n",
    "                \n",
    "        out = model(inputs.to(device))\n",
    "        model_acc, _, _ = compute_acc_without_pad(\n",
    "            out, targets.unsqueeze(1).to(device))\n",
    "\n",
    "        loss = criterion(out, targets.long()) #ce\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Batch {batch_idx} => loss {loss}')\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        acc.append(model_acc)\n",
    "        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optim.step()\n",
    "        t_r = loss.item()\n",
    "        \n",
    "    train_acc.append(np.mean(acc))\n",
    "\n",
    "    # validation\n",
    "    acc = []\n",
    "    model.eval()\n",
    "    for data in dev_dloader:\n",
    "        inputs = data[\"inputs\"].reshape(-1,input_size)\n",
    "        targets = data[\"is_voice\"].reshape(-1,1).view(-1)\n",
    "        out = model(inputs.to(device))\n",
    "        model_acc, _, _ = compute_acc_without_pad(\n",
    "            out, targets.unsqueeze(1).to(device))\n",
    "        acc.append(model_acc)\n",
    "    valid_acc.append(np.mean(acc))\n",
    "    print(f\"epoch: {epoch}, train acc: {train_acc[-1]:.3f}, dev acc: {valid_acc[-1]:.3f}, loss:{t_r:.3f}\")\n",
    "    torch.save({'epoch': epoch + 1, 'state_dict': model.state_dict(),\n",
    "        'optimizer': optim.state_dict()},\n",
    "        f'{log_dir}/checkpoint_{epoch}.pth')\n",
    "                    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batch 0 => loss 0.6250870227813721\n",
      "Batch 10 => loss 0.27857843041419983\n",
      "Batch 20 => loss 0.2462901622056961\n",
      "Batch 30 => loss 0.20429855585098267\n",
      "epoch: 0, train acc: 89.303, dev acc: 92.000, loss:0.197\n",
      "Batch 0 => loss 0.24231553077697754\n",
      "Batch 10 => loss 0.2113974392414093\n",
      "Batch 20 => loss 0.22007986903190613\n",
      "Batch 30 => loss 0.1873924285173416\n",
      "epoch: 1, train acc: 92.545, dev acc: 92.250, loss:0.177\n",
      "Batch 0 => loss 0.23180542886257172\n",
      "Batch 10 => loss 0.20052513480186462\n",
      "Batch 20 => loss 0.2121405452489853\n",
      "Batch 30 => loss 0.1819898933172226\n",
      "epoch: 2, train acc: 92.727, dev acc: 92.250, loss:0.170\n",
      "Batch 0 => loss 0.22829879820346832\n",
      "Batch 10 => loss 0.19605016708374023\n",
      "Batch 20 => loss 0.20944048464298248\n",
      "Batch 30 => loss 0.17929421365261078\n",
      "epoch: 3, train acc: 92.758, dev acc: 92.500, loss:0.166\n",
      "Batch 0 => loss 0.22661226987838745\n",
      "Batch 10 => loss 0.19352906942367554\n",
      "Batch 20 => loss 0.2072865068912506\n",
      "Batch 30 => loss 0.17755171656608582\n",
      "epoch: 4, train acc: 92.879, dev acc: 92.500, loss:0.163\n",
      "Batch 0 => loss 0.22453606128692627\n",
      "Batch 10 => loss 0.1918788105249405\n",
      "Batch 20 => loss 0.205587700009346\n",
      "Batch 30 => loss 0.1762382686138153\n",
      "epoch: 5, train acc: 93.030, dev acc: 92.500, loss:0.161\n",
      "Batch 0 => loss 0.2230684757232666\n",
      "Batch 10 => loss 0.19065646827220917\n",
      "Batch 20 => loss 0.20405739545822144\n",
      "Batch 30 => loss 0.17522387206554413\n",
      "epoch: 6, train acc: 93.091, dev acc: 92.500, loss:0.160\n",
      "Batch 0 => loss 0.22151927649974823\n",
      "Batch 10 => loss 0.1896231323480606\n",
      "Batch 20 => loss 0.20274312794208527\n",
      "Batch 30 => loss 0.1743261218070984\n",
      "epoch: 7, train acc: 93.152, dev acc: 92.500, loss:0.159\n",
      "Batch 0 => loss 0.22044599056243896\n",
      "Batch 10 => loss 0.18877720832824707\n",
      "Batch 20 => loss 0.20160621404647827\n",
      "Batch 30 => loss 0.17359110713005066\n",
      "epoch: 8, train acc: 93.182, dev acc: 92.500, loss:0.158\n",
      "Batch 0 => loss 0.21909254789352417\n",
      "Batch 10 => loss 0.18792317807674408\n",
      "Batch 20 => loss 0.20047296583652496\n",
      "Batch 30 => loss 0.17295695841312408\n",
      "epoch: 9, train acc: 93.182, dev acc: 92.500, loss:0.157\n",
      "Batch 0 => loss 0.21795031428337097\n",
      "Batch 10 => loss 0.1872803419828415\n",
      "Batch 20 => loss 0.19947779178619385\n",
      "Batch 30 => loss 0.17237193882465363\n",
      "epoch: 10, train acc: 93.273, dev acc: 92.500, loss:0.156\n",
      "Batch 0 => loss 0.216840922832489\n",
      "Batch 10 => loss 0.18654634058475494\n",
      "Batch 20 => loss 0.1985166370868683\n",
      "Batch 30 => loss 0.17184200882911682\n",
      "epoch: 11, train acc: 93.333, dev acc: 92.500, loss:0.156\n",
      "Batch 0 => loss 0.21591933071613312\n",
      "Batch 10 => loss 0.18592512607574463\n",
      "Batch 20 => loss 0.19765320420265198\n",
      "Batch 30 => loss 0.17136217653751373\n",
      "epoch: 12, train acc: 93.333, dev acc: 92.500, loss:0.155\n",
      "Batch 0 => loss 0.21492329239845276\n",
      "Batch 10 => loss 0.18541091680526733\n",
      "Batch 20 => loss 0.1966768056154251\n",
      "Batch 30 => loss 0.17090046405792236\n",
      "epoch: 13, train acc: 93.333, dev acc: 92.750, loss:0.155\n",
      "Batch 0 => loss 0.2139071375131607\n",
      "Batch 10 => loss 0.18473485112190247\n",
      "Batch 20 => loss 0.19591188430786133\n",
      "Batch 30 => loss 0.17044208943843842\n",
      "epoch: 14, train acc: 93.333, dev acc: 92.750, loss:0.154\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('vad': conda)"
  },
  "interpreter": {
   "hash": "d889b5b8029a42385c8853d77d52b7bb9562854fc5ba7ad04d6da216dea4a0d2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}