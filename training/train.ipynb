{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "project_dir = \"/Users/joannarownicka/software/vad\"\n",
    "os.chdir(project_dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from lhotse import LilcomFilesWriter\n",
    "\n",
    "from lhotse.features import Fbank, FeatureSetBuilder\n",
    "from lhotse.cut import CutSet, SupervisionSet\n",
    "from lhotse.dataset.sampling import SingleCutSampler, BucketingSampler\n",
    "from lhotse.dataset.vad import VadDataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from preprocessing import prepare_vad_dataset\n",
    "from inference.postprocess import smooth_predictions\n",
    "from models.dnn import DNN\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "root_dir = Path('evaluation/data')\n",
    "corpus_dir = root_dir / 'vad_data/'\n",
    "output_dir = root_dir / 'vad_data_nb/'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "cuts = CutSet.from_json(output_dir / 'cuts_80.json.gz')\n",
    "cuts.describe()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cuts count: 957\n",
      "Total duration (hours): 3.3\n",
      "Speech duration (hours): 2.6 (80.6%)\n",
      "***\n",
      "Duration statistics (seconds):\n",
      "mean    12.3\n",
      "std      3.9\n",
      "min      1.4\n",
      "25%     11.1\n",
      "50%     13.9\n",
      "75%     15.1\n",
      "max     17.2\n",
      "dtype: float64\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Shuffle data but keep seed fixed, split into 80/10/10\n",
    "#cuts_train, cuts_dev_eval = train_test_split(cuts, train_size=0.8, random_state=0)\n",
    "#cuts_dev, cuts_eval = train_test_split(cuts_dev_eval, train_size=0.5, random_state=0)\n",
    "\n",
    "cuts = cuts.shuffle()\n",
    "vad_manifests = prepare_vad_dataset.prepare_vad_dataset(corpus_dir, output_dir)\n",
    "\n",
    "train_ratio = 0.8\n",
    "\n",
    "num_total = len(vad_manifests[\"supervisions\"])\n",
    "stop_train_idx = int(np.floor(num_total * train_ratio))\n",
    "stop_dev_idx = int((num_total - stop_train_idx) // 2 + stop_train_idx)\n",
    "\n",
    "train_ids, dev_ids, eval_ids = [], [], []\n",
    "counter = 0\n",
    "for sup_seg in vad_manifests[\"supervisions\"]:\n",
    "    id = sup_seg.to_dict()[\"id\"]\n",
    "    if counter < stop_train_idx:\n",
    "        train_ids.append(id)\n",
    "    elif counter < stop_dev_idx:\n",
    "        dev_ids.append(id)\n",
    "    else:\n",
    "        eval_ids.append(id)\n",
    "    counter += 1\n",
    "\n",
    "assert train_ids[-1] != dev_ids[0]\n",
    "assert dev_ids[-1] != eval_ids[0]\n",
    "\n",
    "cuts_train = cuts.subset(supervision_ids=train_ids)\n",
    "cuts_dev = cuts.subset(supervision_ids=dev_ids)\n",
    "cuts_eval = cuts.subset(supervision_ids=eval_ids)\n",
    "\n",
    "cuts_train.describe()\n",
    "cuts_dev.describe()\n",
    "cuts_eval.describe()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cuts count: 766\n",
      "Total duration (hours): 2.6\n",
      "Speech duration (hours): 2.1 (80.6%)\n",
      "***\n",
      "Duration statistics (seconds):\n",
      "mean    12.4\n",
      "std      3.9\n",
      "min      1.6\n",
      "25%     11.2\n",
      "50%     13.9\n",
      "75%     15.1\n",
      "max     16.9\n",
      "dtype: float64\n",
      "Cuts count: 95\n",
      "Total duration (hours): 0.3\n",
      "Speech duration (hours): 0.3 (79.9%)\n",
      "***\n",
      "Duration statistics (seconds):\n",
      "mean    12.4\n",
      "std      4.1\n",
      "min      1.4\n",
      "25%     11.3\n",
      "50%     13.9\n",
      "75%     15.1\n",
      "max     17.2\n",
      "dtype: float64\n",
      "Cuts count: 98\n",
      "Total duration (hours): 0.3\n",
      "Speech duration (hours): 0.3 (79.4%)\n",
      "***\n",
      "Duration statistics (seconds):\n",
      "mean    12.2\n",
      "std      4.0\n",
      "min      2.1\n",
      "25%      9.5\n",
      "50%     13.8\n",
      "75%     15.1\n",
      "max     16.4\n",
      "dtype: float64\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "vad_dataset = VadDataset()\n",
    "\n",
    "train_sampler = SingleCutSampler(cuts_train, shuffle=False, max_duration=300)\n",
    "dev_sampler = SingleCutSampler(cuts_dev, shuffle=False, max_duration=300)\n",
    "eval_sampler = SingleCutSampler(cuts_eval, shuffle=False, max_duration=300)\n",
    "\n",
    "train_dloader = DataLoader(vad_dataset, sampler=train_sampler, batch_size=None)\n",
    "dev_dloader = DataLoader(vad_dataset, sampler=dev_sampler, batch_size=None)\n",
    "eval_dloader = DataLoader(vad_dataset, sampler=eval_sampler, batch_size=None)\n",
    "\n",
    "cut_ids = next(iter(dev_sampler))\n",
    "sample = vad_dataset[cut_ids]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-4\n",
    "\n",
    "model_id = \"dnn_ce_fbank80_ignoreindex\"\n",
    "input_size = sample['inputs'][0].shape[1]\n",
    "log_dir = Path('storage/models') / model_id\n",
    "\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = DNN(input_size=input_size, hidden_size=256, num_classes=2).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "#optim = torch.optim.Adam(model.parameters())\n",
    "#                       lr=learning_rate, weight_decay=weight_decay)\n",
    "                        \n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                        weight_decay=weight_decay)\n",
    "\n",
    "def binary_acc(y_pred, y_test): \n",
    "    \n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "    \n",
    "    pred_vals = [ float(np.argmax(item)) for item in y_pred_tag.detach() ]\n",
    "    labels = [ float(item) for item in y_test.detach() ]\n",
    "\n",
    "    keep_indexes = []\n",
    "    for idx, label in enumerate(labels):\n",
    "        if label != float(-100):\n",
    "            keep_indexes.append(idx)\n",
    "\n",
    "    pred_vals = [pred_vals[i] for i in keep_indexes]\n",
    "    labels = [labels[i] for i in keep_indexes]\n",
    "\n",
    "    correct_results_sum = sum(a == b for a, b in zip(pred_vals, labels))\n",
    "    acc = correct_results_sum / len(labels)\n",
    "    acc = np.round(acc * 100)\n",
    "\n",
    "    return acc, pred_vals, labels\n",
    "\n",
    "\n",
    "train_acc = []\n",
    "valid_acc = []\n",
    "for epoch in range(15):\n",
    "    # training\n",
    "    acc = []\n",
    "    model.train()\n",
    "    train_dloader.sampler.set_epoch(epoch)\n",
    "\n",
    "    for batch_idx, data in enumerate(train_dloader):\n",
    "\n",
    "        inputs = data[\"inputs\"].reshape(-1,input_size)\n",
    "        targets = data[\"is_voice\"].reshape(-1,1).view(-1)\n",
    "                \n",
    "        out = model(inputs.to(device))\n",
    "        model_acc, _, _ = binary_acc(out, targets.unsqueeze(1).to(device))\n",
    "\n",
    "        loss = criterion(out, targets.long()) #ce\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Batch {batch_idx} => loss {loss}')\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        acc.append(model_acc)\n",
    "        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optim.step()\n",
    "        t_r = loss.item()\n",
    "        \n",
    "    train_acc.append(np.mean(acc))\n",
    "\n",
    "    # validation\n",
    "    acc = []\n",
    "    model.eval()\n",
    "    for data in dev_dloader:\n",
    "        inputs = data[\"inputs\"].reshape(-1,input_size)\n",
    "        targets = data[\"is_voice\"].reshape(-1,1).view(-1)\n",
    "        out = model(inputs.to(device))\n",
    "        model_acc, _, _ = binary_acc(out, targets.unsqueeze(1).to(device))\n",
    "        acc.append(model_acc)\n",
    "    valid_acc.append(np.mean(acc))\n",
    "    print(f\"epoch: {epoch}, train acc: {train_acc[-1]:.3f}, dev acc: {valid_acc[-1]:.3f}, loss:{t_r:.3f}\")\n",
    "    torch.save({'epoch': epoch + 1, 'state_dict': model.state_dict(),\n",
    "        'optimizer': optim.state_dict()},\n",
    "        f'{log_dir}/checkpoint_{epoch}.pth')\n",
    "                    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batch 0 => loss 0.7127519845962524\n",
      "Batch 10 => loss 0.6825496554374695\n",
      "Batch 20 => loss 0.6679540872573853\n",
      "Batch 30 => loss 0.641040563583374\n",
      "epoch: 0, train acc: 36.697, dev acc: 34.750, loss:0.662\n",
      "Batch 0 => loss 0.6367776393890381\n",
      "Batch 10 => loss 0.6134581565856934\n",
      "Batch 20 => loss 0.5999336838722229\n",
      "Batch 30 => loss 0.5827968716621399\n",
      "epoch: 1, train acc: 44.212, dev acc: 47.500, loss:0.603\n",
      "Batch 0 => loss 0.5782796740531921\n",
      "Batch 10 => loss 0.562932014465332\n",
      "Batch 20 => loss 0.5495798587799072\n",
      "Batch 30 => loss 0.5403707027435303\n",
      "epoch: 2, train acc: 52.000, dev acc: 55.500, loss:0.561\n",
      "Batch 0 => loss 0.5355778932571411\n",
      "Batch 10 => loss 0.5253332257270813\n",
      "Batch 20 => loss 0.5119454264640808\n",
      "Batch 30 => loss 0.5082299709320068\n",
      "epoch: 3, train acc: 59.061, dev acc: 62.000, loss:0.528\n",
      "Batch 0 => loss 0.5028893947601318\n",
      "Batch 10 => loss 0.49604931473731995\n",
      "Batch 20 => loss 0.4823342263698578\n",
      "Batch 30 => loss 0.4822128713130951\n",
      "epoch: 4, train acc: 65.091, dev acc: 67.750, loss:0.502\n",
      "Batch 0 => loss 0.4763542413711548\n",
      "Batch 10 => loss 0.4720372259616852\n",
      "Batch 20 => loss 0.45845717191696167\n",
      "Batch 30 => loss 0.4607129991054535\n",
      "epoch: 5, train acc: 70.000, dev acc: 72.250, loss:0.481\n",
      "Batch 0 => loss 0.45445752143859863\n",
      "Batch 10 => loss 0.4519992470741272\n",
      "Batch 20 => loss 0.43892309069633484\n",
      "Batch 30 => loss 0.44275951385498047\n",
      "epoch: 6, train acc: 73.848, dev acc: 75.500, loss:0.463\n",
      "Batch 0 => loss 0.43609127402305603\n",
      "Batch 10 => loss 0.4350929260253906\n",
      "Batch 20 => loss 0.42253950238227844\n",
      "Batch 30 => loss 0.4275427460670471\n",
      "epoch: 7, train acc: 76.879, dev acc: 78.250, loss:0.447\n",
      "Batch 0 => loss 0.42053964734077454\n",
      "Batch 10 => loss 0.4206647574901581\n",
      "Batch 20 => loss 0.40863215923309326\n",
      "Batch 30 => loss 0.4144271910190582\n",
      "epoch: 8, train acc: 79.212, dev acc: 80.000, loss:0.434\n",
      "Batch 0 => loss 0.40712079405784607\n",
      "Batch 10 => loss 0.4081321954727173\n",
      "Batch 20 => loss 0.3966551125049591\n",
      "Batch 30 => loss 0.4029753804206848\n",
      "epoch: 9, train acc: 80.970, dev acc: 82.000, loss:0.423\n",
      "Batch 0 => loss 0.39535102248191833\n",
      "Batch 10 => loss 0.3971398174762726\n",
      "Batch 20 => loss 0.38621529936790466\n",
      "Batch 30 => loss 0.3928855359554291\n",
      "epoch: 10, train acc: 82.303, dev acc: 83.000, loss:0.413\n",
      "Batch 0 => loss 0.38494887948036194\n",
      "Batch 10 => loss 0.3873896598815918\n",
      "Batch 20 => loss 0.3770299255847931\n",
      "Batch 30 => loss 0.38392505049705505\n",
      "epoch: 11, train acc: 83.303, dev acc: 83.500, loss:0.404\n",
      "Batch 0 => loss 0.37567463517189026\n",
      "Batch 10 => loss 0.37866446375846863\n",
      "Batch 20 => loss 0.36889365315437317\n",
      "Batch 30 => loss 0.375898152589798\n",
      "epoch: 12, train acc: 84.212, dev acc: 84.500, loss:0.396\n",
      "Batch 0 => loss 0.36735397577285767\n",
      "Batch 10 => loss 0.3708094656467438\n",
      "Batch 20 => loss 0.3616493344306946\n",
      "Batch 30 => loss 0.36866581439971924\n",
      "epoch: 13, train acc: 84.848, dev acc: 85.000, loss:0.389\n",
      "Batch 0 => loss 0.35984286665916443\n",
      "Batch 10 => loss 0.363693505525589\n",
      "Batch 20 => loss 0.3551332950592041\n",
      "Batch 30 => loss 0.36210063099861145\n",
      "epoch: 14, train acc: 85.455, dev acc: 85.750, loss:0.382\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "pred_list = []\n",
    "gold_list = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in eval_dloader:\n",
    "        \n",
    "        inputs_eval = data[\"inputs\"].reshape(-1,input_size)\n",
    "        targets_eval = data[\"is_voice\"].reshape(-1,1).view(-1)\n",
    "\n",
    "        out = model(inputs_eval.to(device))\n",
    "        model_acc, predictions, labels = binary_acc(out, targets_eval.unsqueeze(1).to(device))\n",
    "\n",
    "        pred_list.extend(predictions)\n",
    "        gold_list.extend(labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "smoothed_pred_list = smooth_predictions(\n",
    "                pred_list, \\\n",
    "                smooth_num_frames=3, \\\n",
    "                hangover=True, \\\n",
    "                min_speech_frames=10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "print(set(gold_list))\n",
    "print(set(smoothed_pred_list))\n",
    "\n",
    "print(confusion_matrix(gold_list, smoothed_pred_list))\n",
    "print(classification_report(gold_list, smoothed_pred_list))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{0.0, 1.0}\n",
      "{0.0, 1.0}\n",
      "[[19965  4701]\n",
      " [ 6520 88499]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.81      0.78     24666\n",
      "         1.0       0.95      0.93      0.94     95019\n",
      "\n",
      "    accuracy                           0.91    119685\n",
      "   macro avg       0.85      0.87      0.86    119685\n",
      "weighted avg       0.91      0.91      0.91    119685\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "label_height = 10\n",
    "num_frames = 500\n",
    "\n",
    "assert num_frames <= len(pred_list)\n",
    "\n",
    "vad_label = torch.stack([torch.Tensor(pred_list[:num_frames]) for i in range(label_height)]).reshape(label_height, num_frames)\n",
    "plt.matshow(vad_label)\n",
    "\n",
    "vad_label = torch.stack([torch.Tensor(smoothed_pred_list[:num_frames]) for i in range(label_height)]).reshape(label_height, num_frames)\n",
    "plt.matshow(vad_label)\n",
    "\n",
    "vad_label = torch.stack([torch.Tensor(gold_list[:num_frames]) for i in range(label_height)]).reshape(label_height, num_frames)\n",
    "plt.matshow(vad_label)\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ffb4fe006a0>"
      ]
     },
     "metadata": {},
     "execution_count": 71
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('vad': conda)"
  },
  "interpreter": {
   "hash": "d889b5b8029a42385c8853d77d52b7bb9562854fc5ba7ad04d6da216dea4a0d2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}